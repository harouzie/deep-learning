{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = \"One notable feature of the T5 architecture is that the same architecture is used for all the different tasks the model is trained on, ranging from language translation to summarization to question answering\"\n",
    "sent2 = \"The T5 architecture is unique for using the same design for all of the many tasks the model is trained on, including language translation, summarization, and question answering.\"\n",
    "\n",
    "sentence1 = [\n",
    "    \"One notable feature of the T5 architecture is that the same architecture is used for all the different tasks the model is trained on, ranging from language translation to summarization to question answering\",\n",
    "    \"BLEU score is a metric for evaluating a generated sentence to a reference sentence. It is used to measure the quality of machine translation or text summarization\",\n",
    "    \"I love you\"\n",
    "]\n",
    "\n",
    "sentence2 = [\n",
    "    \"The T5 architecture is unique for using the same design for all of the many tasks the model is trained on, including language translation, summarization, and question answering.\",\n",
    "    \"A statistic for comparing a produced sentence to a reference sentence is the BLEU score. It is employed to evaluate how well text summarization or machine translation is done.\",\n",
    "    \"you hate me?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def pinc(sources, references, order=4) -> float:\n",
    "    \"\"\"\n",
    "    @function: PINC: Paraphrase In Ngrams Comparision\n",
    "    @params\n",
    "        sources: list of origin sentences \n",
    "        references: list of paraphrased sentences \n",
    "        order: n-grams that will be measured in PINC score\n",
    "    @return: float - pinc score on order-gram\n",
    "    \"\"\"\n",
    "    if len(sources) != len(references):\n",
    "        raise Exception(\"Number of source sent and refs must be the same\")\n",
    "    batch_score = 0.0\n",
    "\n",
    "    if len(sources[0].split()) < 4:\n",
    "        order = len(sources[0].split())\n",
    "\n",
    "    for src, ref in list(zip(sources, references)):\n",
    "        sent_score = 0.0\n",
    "\n",
    "        for i in range (1, order+1):     \n",
    "            # print(f\"order {i}\")\n",
    "            order_score = 0.0\n",
    "            s1grams = nltk.ngrams(word_tokenize(src), i)\n",
    "            s2grams = nltk.ngrams(word_tokenize(ref), i)\n",
    "            set1 = set(s1grams)\n",
    "            set2 = set(s2grams)\n",
    "            order_score = 1 - float(len(set1.intersection(set2))) / len(set2)\n",
    "            # print(order_score)\n",
    "            sent_score += order_score\n",
    "            \n",
    "        sent_score /= order\n",
    "        batch_score += sent_score\n",
    "        # print(\"sent pinc\",sent_score)\n",
    "        # print(\"batch cumul\", batch_score)\n",
    "    return batch_score / len(sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6376016086249678"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinc(sources=[sent1], references=[sent2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7625872357157117"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinc(sources=sentence1, references=sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'love', 'you')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src = \"I love you\"\n",
    "list(nltk.ngrams(src.split(), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39m1.0\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m0.0\u001b[39;49m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "1.0/0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['One',\n",
       " 'notable',\n",
       " 'feature',\n",
       " 'of',\n",
       " 'the',\n",
       " 'T5',\n",
       " 'architecture',\n",
       " 'is',\n",
       " 'that',\n",
       " 'the',\n",
       " 'same',\n",
       " 'architecture',\n",
       " 'is',\n",
       " 'used',\n",
       " 'for',\n",
       " 'all',\n",
       " 'the',\n",
       " 'different',\n",
       " 'tasks',\n",
       " 'the',\n",
       " 'model',\n",
       " 'is',\n",
       " 'trained',\n",
       " 'on',\n",
       " ',',\n",
       " 'ranging',\n",
       " 'from',\n",
       " 'language',\n",
       " 'translation',\n",
       " 'to',\n",
       " 'summarization',\n",
       " 'to',\n",
       " 'question',\n",
       " 'answering']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.tokenize.word_tokenize(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
